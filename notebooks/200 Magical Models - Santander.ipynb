{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# LOAD LIBRARIES\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd, numpy as np, gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport statsmodels.api as sm\n\n# GET INDICIES OF REAL TEST DATA FOR FE\n#######################\n# TAKE FROM YAG320'S KERNEL\n# https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n\ntest_path = '../input/test.csv'\n\ndf_test = pd.read_csv(test_path)\ndf_test.drop(['ID_code'], axis=1, inplace=True)\ndf_test = df_test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in range(df_test.shape[1]):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv('../input/train.csv', dtype=d)\ntest = pd.read_csv('../input/test.csv', dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE\ndef encode_FE(df,col,test):\n    cv = df[col].value_counts()\n    nm = col+'_FE'\n    df[nm] = df[col].map(cv)\n    test[nm] = test[col].map(cv)\n    test[nm].fillna(0,inplace=True)\n    if cv.max()<=255:\n        df[nm] = df[nm].astype('uint8')\n        test[nm] = test[nm].astype('uint8')\n    else:\n        df[nm] = df[nm].astype('uint16')\n        test[nm] = test[nm].astype('uint16')        \n    return\n\ntest['target'] = -1\ncomb = pd.concat([train,test.loc[real_samples_indexes]],axis=0,sort=True)\nfor i in range(200): encode_FE(comb,'var_'+str(i),test)\ntrain = comb[:len(train)]; del comb\nprint('Added 200 new magic features!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGBM TRAINING AND PREDICTIONS"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# LGBM PARAMETERS\nparam = {\n    'learning_rate': 0.04,\n    'num_leaves': 3,\n    'metric':'auc',\n    'boost_from_average':'false',\n    'feature_fraction': 1.0,\n    'max_depth': -1,\n    'objective': 'binary',\n    'verbosity': -10}\ntrain2 = train.sample(frac=1,random_state=42)\nevals_result = {}\nnum_vars = 200\n\n# SAVE OUT-OF-FOLD PREDICTIONS\nall_oof = np.zeros((len(train2),num_vars+1))\nall_oof[:,0] = np.ones(len(train2))\nall_oofB = np.zeros((len(train2),num_vars+1))\nall_oofB[:,0] = np.ones(len(train2))\n\n# SAVE TEST PREDICTIONS\nall_preds = np.zeros((len(test),num_vars+1))\nall_preds[:,0] = np.ones(len(test))\nall_predsB = np.zeros((len(test),num_vars+1))\nall_predsB[:,0] = np.ones(len(test))\n\nfor j in range(num_vars):\n    \n    # MODEL WITH MAGIC\n    features = ['var_'+str(j),'var_'+str(j)+'_FE']\n    oof = np.zeros(len(train2))\n    preds = np.zeros(len(test))\n    \n    # PLOT DENSITIES    \n    plt.figure(figsize=(16,5))\n    plt.subplot(1,2,2)\n    sns.distplot(train2[train2['target']==0]['var_'+str(j)], label = 't=0')\n    sns.distplot(train2[train2['target']==1]['var_'+str(j)], label = 't=1')\n    plt.legend()\n    plt.yticks([])\n    plt.xlabel('Var_'+str(j))\n\n    # MAKE A GRID OF POINTS FOR LGBM TO PREDICT    \n    mn,mx = plt.xlim()\n    mnFE = train2['var_'+str(j)+'_FE'].min()\n    mxFE = train2['var_'+str(j)+'_FE'].max()\n    step = 50\n    stepB = train2['var_'+str(j)+'_FE'].nunique()\n    w = (mx-mn)/step\n    x = w * (np.arange(0,step)+0.5) + mn\n    x2 = np.array([])\n    for i in range(stepB):\n        x2 = np.concatenate([x,x2])\n    df = pd.DataFrame({'var_'+str(j):x2})\n    df['var_'+str(j)+'_FE'] = mnFE + (mxFE-mnFE)/(stepB-1) * (df.index//step)\n    df['pred'] = 0\n    \n    # 5-FOLD WITH MAGIC\n    for k in range(5):\n            valid = train2.iloc[k*40000:(k+1)*40000]\n            train = train2[ ~train2.index.isin(valid.index) ]    \n            trn_data  = lgb.Dataset(train[features], label=train['target'])\n            val_data = lgb.Dataset(valid[features], label=valid['target'])     \n            model = lgb.train(param, trn_data, 750, valid_sets = [trn_data, val_data], \n                    verbose_eval=False, evals_result=evals_result)      \n            x = evals_result['valid_1']['auc']\n            best = x.index(max(x))\n            #print('i=',i,'k=',k,'best=',best)\n            oof[k*40000:(k+1)*40000] = model.predict(valid[features], num_iteration=best)\n            preds += model.predict(test[features], num_iteration=best)/5.0\n            df['pred'] += model.predict(df[features], num_iteration=best)/5.0\n            \n    val_auc = roc_auc_score(train2['target'],oof)\n    print('VAR_'+str(j)+' with magic val_auc =',round(val_auc,5))\n    all_oof[:,j+1] = oof\n    all_preds[:,j+1] = preds\n    x = df['pred'].values\n    x = np.reshape(x,(stepB,step))\n    x = np.flip(x,axis=0)\n    \n    # PLOT LGBM PREDICTIONS USING MAGIC    \n    plt.subplot(1,2,1)\n    sns.heatmap(x, cmap='RdBu_r', center=0.0) \n    plt.title('VAR_'+str(j)+' Predictions with Magic',fontsize=16)    \n    plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))\n    plt.xlabel('Var_'+str(j))\n    s = min(mxFE-mnFE+1,20)\n    plt.yticks(np.linspace(mnFE,mxFE,s)-0.5,np.linspace(mxFE,mnFE,s).astype('int'))\n    plt.ylabel('Count')\n    plt.show()\n    \n    # MODEL WITHOUT MAGIC\n    features = ['var_'+str(j)]\n    oof = np.zeros(len(train2))\n    preds = np.zeros(len(test))\n    \n    # PLOT DENSITIES\n    plt.figure(figsize=(16,5))\n    plt.subplot(1,2,2)\n    sns.distplot(train2[train2['target']==0]['var_'+str(j)], label = 't=0')\n    sns.distplot(train2[train2['target']==1]['var_'+str(j)], label = 't=1')\n    plt.legend()\n    plt.yticks([])\n    plt.xlabel('Var_'+str(j))\n    \n    # MAKE A GRID OF POINTS FOR LGBM TO PREDICT\n    mn,mx = plt.xlim()\n    mnFE = train2['var_'+str(j)+'_FE'].min()\n    mxFE = train2['var_'+str(j)+'_FE'].max()\n    step = 50\n    stepB = train2['var_'+str(j)+'_FE'].nunique()\n    w = (mx-mn)/step\n    x = w * (np.arange(0,step)+0.5) + mn\n    x2 = np.array([])\n    for i in range(stepB):\n        x2 = np.concatenate([x,x2])\n    df = pd.DataFrame({'var_'+str(j):x2})\n    df['var_'+str(j)+'_FE'] = mnFE + (mxFE-mnFE)/(stepB-1) * (df.index//step)\n    df['pred'] = 0\n    \n    # 5-FOLD WITHOUT MAGIC\n    for k in range(5):\n            valid = train2.iloc[k*40000:(k+1)*40000]\n            train = train2[ ~train2.index.isin(valid.index) ]\n            trn_data  = lgb.Dataset(train[features], label=train['target'])\n            val_data = lgb.Dataset(valid[features], label=valid['target'])     \n            model = lgb.train(param, trn_data, 750, valid_sets = [trn_data, val_data], \n                    verbose_eval=False, evals_result=evals_result)      \n            x = evals_result['valid_1']['auc']\n            best = x.index(max(x))\n            #print('i=',i,'k=',k,'best=',best)\n            oof[k*40000:(k+1)*40000] = model.predict(valid[features], num_iteration=best)\n            preds += model.predict(test[features], num_iteration=best)/5.0\n            df['pred'] += model.predict(df[features], num_iteration=best)/5.0\n            \n    val_auc = roc_auc_score(train2['target'],oof)\n    print('VAR_'+str(j)+' without magic val_auc =',round(val_auc,5))\n    all_oofB[:,j+1] = oof\n    all_predsB[:,j+1] = preds\n    x = df['pred'].values\n    x = np.reshape(x,(stepB,step))\n    x = np.flip(x,axis=0)\n    \n    # PLOT LGBM PREDICTIONS WITHOUT USING MAGIC\n    plt.subplot(1,2,1)\n    sns.heatmap(x, cmap='RdBu_r', center=0.0) \n    plt.title('VAR_'+str(j)+' Predictions without Magic',fontsize=16)\n    plt.xticks(np.linspace(0,49,5),np.round(np.linspace(mn,mx,5),1))\n    plt.xlabel('Var_'+str(j))\n    plt.yticks([])\n    plt.ylabel('')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble 200 Models with LR\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ENSEMBLE MODEL WITHOUT MAGIC\nlogrB = sm.Logit(train2['target'], all_oofB[:,:num_vars+1])\nlogrB = logrB.fit(disp=0)\nensemble_predsB = logrB.predict(all_oofB[:,:num_vars+1])\nensemble_aucB = roc_auc_score(train2['target'],ensemble_predsB)  \nprint('##################')\nprint('Combined Model without magic Val_AUC=',round(ensemble_aucB,5))\nprint()\n\n# ENSEMBLE MODEL WITH MAGIC\nlogr = sm.Logit(train2['target'], all_oof[:,:num_vars+1])\nlogr = logr.fit(disp=0)\nensemble_preds = logr.predict(all_oof[:,:num_vars+1])\nensemble_auc = roc_auc_score(train2['target'],ensemble_preds)  \nprint('##################')\nprint('Combined Model with magic Val_AUC=',round(ensemble_auc,5))\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# SAVE PREDICTIONS TO CSV    \nprint('Test predictions saved as submission.csv')\nprint('OOF predictions saved as oof_submission.csv')\nprint('Histogram of test predictions displayed below:')\n\nsub = train2[['ID_code','target']].copy()\nsub['predict'] = ensemble_preds\nsub.reset_index(inplace=True)\nsub.sort_values('index',inplace=True)\nsub.to_csv('oof_submission.csv',index=False)\n\ntest_preds = logr.predict(all_preds[:,:num_vars+1])\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = test_preds\nsub.to_csv('submission.csv',index=False)\n\n# DISPLAY HISTOGRAM OF PREDICTIONS\nb = plt.hist(sub['target'], bins=200)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
